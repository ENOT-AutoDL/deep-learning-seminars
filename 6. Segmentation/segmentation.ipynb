{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6gP536z0kzW"
   },
   "source": [
    "# Семинар 6. Сегментация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V6fCWymxOk0"
   },
   "source": [
    "1. Сегментация = попиксельная классификация\n",
    "2. Требует меньшего объема тренировочных данных, чем классификация (пара сотен - пара тысяч картинок)\n",
    "3. Только свёртки, нет fc-слоёв → размер изображения любой\n",
    "4. Если для классификации нам нужны были картинки и их классы, то для сегментации нам нужны картинки и их маски (классы для каждого пикселя изображения)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCecil_17xCQ"
   },
   "source": [
    "![](images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5oMWWn9A8hv"
   },
   "source": [
    "Пример маски:\n",
    "\n",
    "![](images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roTFev-RjFKM"
   },
   "source": [
    "Из изображения размера `3xHxW` получаем изображение размера `CxHxW` при помощи сети. Дальше на каждый пиксель можем накинуть `softmax` (или `sigmoid`, если у нас бинарная сегментация) и получить предсказанные сетью вероятности. То есть в сегментации на вход кросс-энтропии идет не выход fc-слоя, а выход conv-слоя\n",
    "\n",
    "![](images/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP6YbVLxIE1P"
   },
   "source": [
    "# Метрики и лосс для сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72wOtM008gKn"
   },
   "source": [
    "## Метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** - имеет те же недостатки, что и для обычной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IoU** (intersection over union), или Jaccard index - самая распространенная\n",
    "\n",
    "Наглядная иллюстрация IOU:\n",
    "![](images/2.png)\n",
    "\n",
    "$IoU = \\frac{|X∩Y|}{|X∪Y|}$,\n",
    "`X` - это gt-маска, а `Y` - это маска, предсказанная сетью\n",
    "\n",
    "Например, в случае бинарной сегментации (1 - объект, 0 - фон) мы можем получить предсказанную сетью маску так: `y_mask = (y_pred > 0.5).float()`, здесь `y_pred` - это выход сигмоиды (то есть предсказанная сетью вероятность), а `0.5` - это наш порог для предсказания.\n",
    "  \n",
    "*Упражнение 1*: Выразите формулу $IOU$ через маски `y_pred` и `y_gt`\n",
    "\n",
    "*Упражнение 2*: Зависит ли IOU от размера сегментируемого объекта? Обоснуйте ответ\n",
    "\n",
    "*Упражнение 3*: Посмотрите на картинку, иллюстрирующую IOU. Один из квадратиков (левый) является gt, а второй - предсказанием. Какая часть картинки является `TP`, `FP`, `TN` и `FN`?\n",
    "  \n",
    "**DICE**\n",
    "\n",
    "$DICE = \\frac{2\\times|X∩Y|}{|X|+|Y|}$\n",
    "\n",
    "*Упражнение 1*: Выразите формулу $DICE$ через маски `y_pred` и `y_gt`\n",
    "\n",
    "*Упражнение 2*: Выразите $DICE$ через $IOU$\n",
    "\n",
    "*Упражнение 3*: Выразите $DICE$ через `precision` и `recall`\n",
    "\n",
    "*Упражнение 4*: У $IOU$ есть наглядная интерпретация (см. картинку). Как интерпретировать $DICE$? Почему у него такая странная формула?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JfmYAJ92TyL"
   },
   "source": [
    "## Лоссы\n",
    "\n",
    "- **Кросс-энтропия** - Подходит для задач классификации, и значит подойдёт для попиксельной классификации\n",
    "- **Jaccard loss** - Похож на $1 - IOU$; минимизируя его, мы явно максимизируем $IOU$\n",
    "- **Dice loss** - Похож на $1 - DICE$; минимизируя его, мы явно максимизируем $DICE$\n",
    "\n",
    "*Упражнение 1*: Какая из этих метрик лучше подходит для задачи сегментации для несбалансированных классов?\n",
    "\n",
    "*Упражнение 2*: Диффиренцируема ли метрика $IOU$? Если да, то посчитайте её производную. Если нет, то как почему можно использовать Jaccard loss в качестве функции потерь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNh85GmoIsnA"
   },
   "source": [
    "# Сети для сегментации. Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIbZCAOf5ugk"
   },
   "source": [
    "- В обычной классификационной сети у нас есть куча свёрток. После сверток карта признаков уменьшается в размере, но у нее становится больше каналов. В конце сети после свёрток есть что-нибудь вроде GAP, схлопывающего пространственные измерения, и fc-слой, переводящий признаки в классы.\n",
    "- Уберем из сети GAP и fc-слои дальше, а оставим только последнюю карту признаков. В этом тензоре уже содержится достаточно много информации об объекте (где примерно он расположен и что примерно из себя представляет)\n",
    "- Можем добавить upsample-слои, которые извлеченные из картинки фичи (малого `hxw` и с большим `c`) переведут в изображение исходного размера\n",
    "- Получится fully convolutional сеть (без fc-слоёв)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGbRo6WgECVm"
   },
   "source": [
    "![](images/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jmfZylkJbMg"
   },
   "source": [
    "FCN = Efficient Sliding Window\n",
    "\n",
    "Можно себе представить сегментацию как обычную классификацию. Представим, что у нас есть скользящее окно. Мы скользим этим окном по картинке, делаем кроп и предсказываем класс центрального пикселя. Но это жутко неэффективно, потому что одни и те же пиксели изображения будут участвовать в свертках при близких кропах.\n",
    "\n",
    "*Иллюстрация справа:*\n",
    "\n",
    "*1. Делаем предсказание для картинки 14x14, пропуская тензор через серию свёрток и нелинейностей - получаем карту признаков размера 1x1*\n",
    "\n",
    "*2. Делаем предсказание для картинки 16x16 таким же образом - получаем карту признаков 4x4. Если бы мы прошлись скользящим окном размера 14x14 по картинке 16x16, то получили бы эти же 4 пикселя*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGZixBERJN94"
   },
   "source": [
    "![](images/6.png)\n",
    "\n",
    "*Картинка из лекций sim0nsays*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVAmZ65hJPl5"
   },
   "source": [
    "## Upsampling\n",
    "\n",
    "Какие есть способы увеличить карту признаков в размере:\n",
    "\n",
    "- Resize Nearest Neighbors\n",
    "- MaxUnpooling\n",
    "- Resize Bilinear\n",
    "- **Transposed convolution** - самая распространенная, потому что есть обучаемые параметры, а значит сеть может выучить какие-то закономерности\n",
    "\n",
    "![](images/7.png)\n",
    "\n",
    "Можем увеличить карту признаков сразу до нужного размера за один раз (один `upsample`), а можем итеративно, сделав много `ConvTranspose` слоёв и нелинейностей между ними. Тогда границы будут определяться чётче"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DN5JOl_g5A8P"
   },
   "source": [
    "![](images/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97NrQUzWg8KI"
   },
   "source": [
    "## FCN\n",
    "\n",
    "В нашей сети 5 страйдов x2, изображение в конце уменьшается в 32 раза. На последнем слое информация о классе очень высокоуровневая, но пространственные границы размыты. На более близком к началу сети уровне фичи более простые, но информация о пространственной координате еще не успела испортиться.\n",
    "\n",
    "- Собираем информацию с последнего слоя pool5, увеличиваем в x32 раза (FCN-32s)\n",
    "\n",
    "- Делаем `upsample` в 2 раза информации из pool5 и фьюзим (суммируем) с pool4 - можно  увеличить в x16 и будет еще одно предсказание (FCN-16s)\n",
    "\n",
    "и т.д.\n",
    "\n",
    "![](images/9.png)\n",
    "\n",
    "![](images/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYzX1eM2Ty9f"
   },
   "source": [
    "## UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn6_EfD2lGfA"
   },
   "source": [
    "- Чем более глубокий слой для `upsampling`'а мы используем, тем более качественные там фичи, но в то же время и хуже сохранилась пространственная информация (сравните `FCN-32s` и `FCN-8s` на картинке выше).\n",
    "- А давайте будем прокидывать skip connection из энкодера в декодер. Благодаря этому нам будет передаваться пространственный контекст. Тут уже не суммирование после skip connection, как в `ResNet`, а конкатенация. После конкатенации есть свёртка, что позволяет выбирать сети, как перемешивать каналы. Можно и просто складывать (есть такая модификация архитектуры)\n",
    "- Skip connection - это всегда хорошо для градиента\n",
    "- `UNet` - это семейство архитектур. [Можно выбрать](https://github.com/qubvel/segmentation_models.pytorch) UNet со своим энкодером (например, `MobileNetV2`)\n",
    "\n",
    "[Пост про UNet на Towards Data Science](https://towardsdatascience.com/u-net-b229b32b4a71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFH0qQAQepJT"
   },
   "source": [
    "![](images/11.png)\n",
    "\n",
    "*Схема оригинального UNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AU1ownixFpq"
   },
   "source": [
    "### Feature Pyramid Network\n",
    "\n",
    "Feaute Pyramid Network (FPN) очень похожа на UNet. Главное отличие в том, что в FPN предсказания делаются на разных масштабах, а потом объединяются. В UNet предсказание делается один раз в конце. То есть даже при одинаковой U-образной архитектуре, в FPN больше свёрток, т.е. эффективно больше глубина\n",
    "\n",
    "![](images/12.png)\n",
    "\n",
    "Feature pyramid часто применяется в сегментации и детекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHQZR77NIuUA"
   },
   "source": [
    "# Разное"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtqCIl196vxR"
   },
   "source": [
    "Несколько моментов, которые еще не упомянули:\n",
    "\n",
    "- Если используете аугментации, то важно, чтобы они были были одинаковыми и для маски, и для изображения (можно использовать `albumentations`)\n",
    "\n",
    "- Два типа сегментации:\n",
    "  - semantic segmentation\n",
    "  - instance segmentation\n",
    "  (см рисунок ниже)\n",
    "\n",
    "  До сих пор мы говорили только о semantic segmentation, для которой два разных объекта одного класса неразличимы. Instance segmentation можно условно представлять себе как детекцию и сегментацию внутри бокса, который вернул детектор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aysMdPY0QhH"
   },
   "source": [
    "![](images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры готовых репозиториев для сегментации:\n",
    "\n",
    "- [segmentation models pytorch](https://github.com/qubvel/segmentation_models.pytorch)\n",
    "- [MSegmentation](https://github.com/open-mmlab/mmsegmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "check_quant",
   "language": "python",
   "name": "check_quant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
