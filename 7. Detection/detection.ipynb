{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEuhmxQjyUX4"
   },
   "source": [
    "# Семинар 7. Детекция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZPmAfpN18wl"
   },
   "source": [
    "![](images/1.jpeg)\n",
    "\n",
    "*Пример детекции*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YVCZcI7l7N2"
   },
   "source": [
    "Детекция - это такая задача, в которой нейронная сеть (детектор) принимает картинку. В результате детекции получается список из задетектированных объектов (детектов) вида\n",
    "\n",
    "$$[(b_x, b_y, b_h, b_w, p_{obj}, p_{1}, ..., p_{k}), ...]$$\n",
    "\n",
    "т.е. координат объекта (бокса), вероятности нахождения объекта в боксе и вероятности принадлежать каждому из $k$ классов\n",
    "\n",
    "Почему мы вообще можем такое делать при помощи свёрточной нейросети?\n",
    "\n",
    "[Andrew Ng об sliding window](https://www.youtube.com/watch?v=5e5pjeojznk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JxTbT-Y2JpE"
   },
   "source": [
    "Детекторы бывают\n",
    "\n",
    "- одностадийные (one-stage)\n",
    "- двустадийные (two-stage)\n",
    "\n",
    "Двустадийные детекторы исторически появились раньше, поэтому начнём обсуждение с них"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzEyjvwR3LCb"
   },
   "source": [
    "# Two-Stage Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKT9miaq4LBf"
   },
   "source": [
    "## RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ8mX9kM-ESP"
   },
   "source": [
    "1. Берётся предварительно обученная CNN (например, ResNet на ImageNet'е)\n",
    "\n",
    "2. По исходному изборажению определяются regions proposals - области, в которых могут содержаться объекты - при помощи алгоритма под названием [selective search](https://www.geeksforgeeks.org/selective-search-for-object-detection-r-cnn/). Selective search делит изображение на сегменты, используя информацию о цвете соседних пикселей и т.п., а потом иерархически их объединяет. Так как нам нужны разные масштабы, получается много боксов\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-08_at_11.44.51_AM_cltn2Mh.png)\n",
    "\n",
    "3. Боксы отдаются на вход свёрточной сети. Свёрточная сеть выдаёт фичи (векторы, которые получаются при прохождении картинкой части CNN до fc-слоя). Дальше происходит классификация по фичам при помощи [Support Vector Machine](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47), уточняются координаты боксов (регрессия) и выдаётся confidence, что в боксе есть объект\n",
    "\n",
    "![](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/RCNN.png)\n",
    "\n",
    "4. NMS (Non-Maximum-Supression) - подавляет лишние детекты (по умолчанию у нас их 2k на каждый region proposal)\n",
    "\n",
    "Недостатки: медленно\n",
    "- Прогоняется CNN для каждого region proposal'а\n",
    "- Генерация 2000 region proposal'ов для каждой картинки при помощи selective search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5zdNnzlIYPM"
   },
   "source": [
    "### NMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3KNozuRIwfX"
   },
   "source": [
    "Модель находит несколько боксов для одного и того же объекта. Мы же хотим на выходе сетки один бокс для одного объекта, причем хорошего качества\n",
    "\n",
    "Алгоритм NMS:\n",
    "\n",
    "1. Отсортировать все боксы по confidence score и отбросить те, у которых он совсем низкий (ниже confidence threshold)\n",
    "2. Для оставшихся боксов (в цикле):\n",
    "  - Выбрать бокс с лучшим confidence score\n",
    "  - Убрать боксы с IOU больше порога (н., IOU > 0.5) с предыдущим боксом\n",
    "\n",
    "![](images/4.jpg)\n",
    "\n",
    "[Andrew Ng объясняет NMS](https://www.youtube.com/watch?v=VAo84c1hQX8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WYsorpi4NGc"
   },
   "source": [
    "## Fast-RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APjoVWDMQLhN"
   },
   "source": [
    "1. Region proposals определяются так же, как и раньше, при помощи selective search - по исходному изображению\n",
    "2. Вся картинка (а не каждый region proposal отдельно) идёт на свёрточную сеть\n",
    "3. Прогоняем всё изображение через сеть и получаем карту признаков\n",
    "4. Из карты признаков \"вырезаем\" тензоры, которые соответствуют region proposal'ам. Для этого используется специальный слой ROI pooling\n",
    "5. Отправляем дальше эти тензоры на классификацию, регрессию координат (классификация уже при помощи fc-слоя, а не SVM)\n",
    "5. NMS\n",
    "\n",
    "Такое простое изменение помогло значительно ускорить детекцию (для примера: 50 секунд → 2 секунды, x25). Теперь самая долгая часть всего этого пайплайна детекции - это selective search\n",
    "\n",
    "![](images/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV403vCy4PXM"
   },
   "source": [
    "## Faster-RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6MCo8tjkyJA"
   },
   "source": [
    "### Anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q92erujk2Nh"
   },
   "source": [
    "![](images/6.png)\n",
    "\n",
    "- Помогают предсказывать разные объекты, которые оказались в одном суперпикселе\n",
    "- Сети будет проще обучиться подгонять регрессией боксы к нужной форме, если задать ей адекватные anchor box'ы\n",
    "- Не очень понятно, как нейронную сеть заставить выдавать выходы произвольного (или меняющегося) размера: один бокс для 1й картинки, 5 боксов для 2й. Когда у нас есть anchor box'ы, мы учим сеть находить предопределенное заранее количество боксов, которые потом мы используем или не используем дальше для обучения (в зависимости от того, попал туда объект или нет)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvEO0lKqWPsp"
   },
   "source": [
    "### Архитектура Faster-RCNN\n",
    "\n",
    "Поскольку теперь самая долгая часть - это selective search, то логичнее всего для ускорения детекции предложить заменить этот алгоритм на более быстрый. Теперь у нас будет отдельная сеть для определения region proposals - Region Proposal Network (RPN). Остальная часть будет как в fast-RCNN. Всё вместе это будет называться Faster-RCNN\n",
    "\n",
    "![](images/7.png)\n",
    "\n",
    "Как работает RPN:\n",
    "\n",
    "1. Скользящее окно движется по карте признаков, общей с fast-RCNN (получается, что region proposals мы ищем уже по карте признаков, а не по исходной картинке)\n",
    "2. Центр скользящего окна связан с центром якорей (anchor box'ов). Используются anchor box'ы разной формы и размеров. Это как бы \"заготовка\" для более точного бокса\n",
    "3. Если IOU anchor box'а с ground truth выше порога (например, 70%), то мы говорим, что это положительный якорь, а если ниже порога (например, 30%) - то это отрицательный якорь. Все остальные якоря не участвуют в обучении\n",
    "\n",
    "  Положительный якорь → значит, бокс соответствует нашему объекту → эти боксы учим регрессией быть более похожими на ground truth. Также учим их выдавать тот же класс, что и ground truth (кросс-энтропия)\n",
    "\n",
    "  Отрицательный якорь → значит, бокс точно не соответствует нашему объекту → эти боксы в регрессии не участвуют. Также учим их выдавать противоположный класс на классификации\n",
    "\n",
    "  **А если вкратце, RPN \"отвечает\" на два вопроса: содержит ли этот якорь объект и, если да, то как нужно поменять координаты якоря, чтобы он соответствовал найденному объекту. Дальше RCNN модуль классифицирует бокс и уточняет его координаты (если объект найден) или говорит, что это фон.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N88R10vgTZ3X"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn3YB3vTTaL_"
   },
   "source": [
    "![](images/8.png)\n",
    "\n",
    "*Two-stage detectors, изображение из блога [lilianweng.github.io](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/). Справа модель Mask-RCNN, которую мы не обсуждаем в рамках данной темы*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EVbdFHC3E2e"
   },
   "source": [
    "# One-Stage Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9nyKzQieiE1"
   },
   "source": [
    "- Ранее детектирование производилось путём выбора областей изображения и их классификации\n",
    "- Модель для детектирования состояла из нескольких частей\n",
    "- Подобные сети требуют большого времени на предсказание, а ещё больше на обучение сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUrKv_-I3Ks_"
   },
   "source": [
    "## SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpVrp3vHkizf"
   },
   "source": [
    "- В качестве бэкбона используется `VGG16`, обученная на `ImageNet`\n",
    "- К бэнбону добавляется серия свёрток\n",
    "- Детекция происходит на каждой из этих свёрток, чтобы лучше работать с разными масштабами\n",
    "- Pre-defined anchors\n",
    "\n",
    "![](images/9.png)\n",
    "\n",
    "Anchor box'ы для разных масштабов:\n",
    "\n",
    "![](images/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1njW592H6nmi"
   },
   "source": [
    "## YOLO - You Only Look Once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pPlXKwluWo_"
   },
   "source": [
    "- Первая версия без anchor box'ов. Изображение делилось на $S\\times S$ клеток, в каждой из которых предсказывалось $B$ штук bounding box'ов, степень уверенности на каждый бокс и $C$ штук вероятностей классов на одну ячейку\n",
    "- В основе лежит `GoogleNet`, в которой `inception`-блоки заменили на conv1x1 + conv3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot-Q3T8JIn4e"
   },
   "source": [
    "После свёрток получаем карту признаков. В двухстадийном детекторе мы сначала находили regions of interest (ROI), а потом уже их уточняли при помощи регрессии. Раз мы всё равно уточняем координаты бокса, то будем поступать проще: поделим изображение на непересекающиеся патчи и каждый из них будем считать нашим ROI. А потом для каждого патча мы уточним бокс:\n",
    "\n",
    "![](images/11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKTjtmGFIvyF"
   },
   "source": [
    "Плюсы:\n",
    "\n",
    "- Работает в один заход, быстрая\n",
    "\n",
    "Минусы:\n",
    "\n",
    "- Каждая ячейка может предсказывать только один класс\n",
    "- Плохо справляется с маленькими объектами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk9GqPV46QtF"
   },
   "source": [
    "![](images/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xN8j5t-wwjU-"
   },
   "source": [
    "### [YOLO](https://arxiv.org/abs/1506.02640) (2015) → [YOLO9000](https://arxiv.org/abs/1612.08242v1) (YOLOv2) (2016) → [YOLOv3](https://arxiv.org/abs/1804.02767v1) (2018) → [YOLOv4](https://arxiv.org/abs/2004.10934v1) (2020) → [YOLOv5](https://github.com/ultralytics/yolov5) (2020)\n",
    "\n",
    "- От предсказания bounding box'ов перешли к предсказанию поправок к anchor'ам\n",
    "- Добавили skip connection'ы\n",
    "- Предсказания производятся для 3 разных скейлов, чтобы лучше детектировать объекты разного размера\n",
    "- Для каждого скейла три anchor box'а\n",
    "- Anchor'ы можно подобрать под датасет\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkCRJCFtFOf1"
   },
   "source": [
    "### Anchor'ы в YOLO\n",
    "\n",
    "Ситуация становится сложнее, когда в одном суперпикселе несколько объектов. Используем для каждой ячейки anchor box'ы.\n",
    "\n",
    "Изображение делится на $S\\times S$ клеток, в каждой из которых есть $A$ штук anchor box'ов. Можно выбрать их по датасету (кластеризация + усреднение), можно задать руками.\n",
    "\n",
    "Во время обучения\n",
    "- каждый объект относится к определенной ячейке\n",
    "- выбирается anchor box, имеющий максимальный IOU с боксом из датасета\n",
    "\n",
    "Выход: $S \\times S$ клеток $\\times$ $A$ якорей $\\times (5 + C)$\n",
    "\n",
    "![](images/12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq8AE-gNGqMM"
   },
   "source": [
    "### Разные скейлы в YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsCB3SUYGidM"
   },
   "source": [
    "![](images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9ohFQk5Cpck"
   },
   "source": [
    "Структура одностадийного детектора. Карты признаков для разных масштабов смешиваются между собой, а после этого происходит непосредственно детекция (вспомним `Feature Pyramid Network`):\n",
    "\n",
    "![](images/14.png)\n",
    "\n",
    "Вместо классического `Feature Pyramid Network` используют `Path Aggreagtion Network` (более хитрое смешивание фичей с разных масштабов):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urW08x4BCzsh"
   },
   "source": [
    "![](images/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lvQ8P1Yc20i"
   },
   "source": [
    "## Loss\n",
    "\n",
    "  $$𝓛 = 𝓛_{reg} + 𝓛_{obj} + 𝓛_{clas}$$\n",
    "  - регрессия: уточняются координаты, штрафуется за сдвиг центра бокса относительно ground truth и за уширение/удлинение бокса по сравнению с ground truth\n",
    "  - object loss: учим сеть выдавать object confidence, близкий к 1, в тех боксах, где есть объект (т.е. отличать объект от шума, от заблюренных пикселей, фона и т.п.)\n",
    "  - классификация: обычная кросс-энтропия\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rHKBpmf2Qua"
   },
   "source": [
    "# mAP (Mean Average Precision) - метрика для детекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2ZYyzz1Ej09"
   },
   "source": [
    "Сложная интегральная метрика, нужно в одном числе учесть и точность боксов, и confidence класса. От того, какие пороги по confidence и по IOU мы поставим, будет зависеть, какие детекты мы получим\n",
    "\n",
    "Как считаем метрику (при фиксированном `IOU_thr` и `conf_thr`):\n",
    "- **TP**: лучший бокс с $IOU(pred, gt) > IOU\\_thr$\n",
    "- **FP**: $IOU(pred, gt) < IOU\\_thr$ или бокс-дубликат\n",
    "- **FN**: не было предсказано бокса либо модель предсказала бокс, но ошиблась с классом\n",
    "\n",
    "  Теперь у нас есть нужные величины, чтобы посчитать `precision` и `recall`\n",
    "\n",
    "*Вопрос: Как выглядят формулы для Precision и Recall?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00o8L8zZLJXd"
   },
   "source": [
    "Хотим, чтобы наша метрика учитывала то, как влияет изменение порогов на изменение предсказаний сети\n",
    "\n",
    "Алгоритм:\n",
    "\n",
    "0. Выбираем один класс, пункты 1 - 4 проделываем для всех классов отдельно\n",
    "1. Фиксируем `IOU_thr`, пробегаемся в цикле по `conf_thr` (`linspace`).\n",
    "2. Для выбранных порогов считаем две цифры: `(precision, recall)`.\n",
    "3. Строим precision-recall curve.\n",
    "4. Считаем площадь под кривой\n",
    "5. Полученная величина называется `Average Precision` и зависит она от IOU-порога: `AP(IOU_thr)`\n",
    "6. Мы можем усреднить полученные для всех классов `AP(IOU_thr)`, назовем такую метрику `mAP(IOU_thr)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разное\n",
    "\n",
    "[Видео о детекторах на русском](https://www.youtube.com/watch?v=LFrvzlcWthk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "check_quant",
   "language": "python",
   "name": "check_quant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
